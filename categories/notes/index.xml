<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Be creative</title>
    <link>http://nianze.tk/categories/notes/</link>
    <description>Recent content in Notes on Be creative</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jun 2019 19:39:39 -0400</lastBuildDate>
    
        <atom:link href="http://nianze.tk/categories/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Basic Musicianship Review</title>
      <link>http://nianze.tk/2019/06/basic-musicianship-review/</link>
      <pubDate>Sat, 15 Jun 2019 19:39:39 -0400</pubDate>
      
      <guid>http://nianze.tk/2019/06/basic-musicianship-review/</guid>
      <description>&lt;p&gt;A review of some basic music theory before the NYU Steinhardt music technology placement exam.

&lt;!-- toc --&gt;&lt;/p&gt;

&lt;h1 id=&#34;intervals&#34;&gt;Intervals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Perfect Intervals: 1(perfect unison), 4(perfect 4th), 5(perfect 5th), 8(perfect octave)&lt;/li&gt;
&lt;li&gt;Major Intervals: 2(major 2nd), 3(major 3rd), 6(major 6th), 7(major 7th)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;scales&#34;&gt;Scales&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;major scale (WWHWWWH)&lt;/li&gt;
&lt;li&gt;natural minor scale / aeolian scale (WHWWHWW) = based off the 6th degree that shares the key signature of the major scale&lt;/li&gt;
&lt;li&gt;harmonic minor scale(WHWWH1.5H) = natural minor + sharpened 7th degree&lt;/li&gt;
&lt;li&gt;melodic minor scale(ASC:WHWWWWH, DESC:WWHWWHW) =  the ascending form of the scale has both a raised 6th and 7th degree, and the descending form of the scale reverts back to the natural minor scale form&lt;/li&gt;
&lt;li&gt;chromatic scale(H*12) = constructed entirely of half steps or semitones.&lt;/li&gt;
&lt;li&gt;wholetone scale(W*6) = constructed entirely of whole-steps or tones.&lt;/li&gt;
&lt;li&gt;lydian scale(WWWHWWH) = based off the 4th degree of any major scale (C major scale from its fourth degree (F) -&amp;gt; F lydian scale)&lt;/li&gt;
&lt;li&gt;mixolydian scale(WWHWWHW) = based off the 5th degree of any major scale (C major scale from its fifth degree (G), -&amp;gt; G mixolydian scale)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;chords&#34;&gt;Chords&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Tonic: the note considered to be the basis of the chord, e.g.: C, D, â€¦&lt;/li&gt;
&lt;li&gt;Quality

&lt;ul&gt;
&lt;li&gt;major chords = 1,3,5 notes of major scale, major 3rd followed by minor 3rd;&lt;/li&gt;
&lt;li&gt;minor chords = 1,3,5 notes of minor scale, minor 3rd followed by major 3rd, same as a major chord but the third being lowered a semitone&lt;/li&gt;
&lt;li&gt;diminished chords = similar to minor, but the top note (the fifth) is also flattened, minor 3rd followed by minor 3rd.&lt;/li&gt;
&lt;li&gt;augmented chords = similar to major, but the top note (the fifth) is raised by a semitone, major 3rd followed by a major 3rd&lt;/li&gt;
&lt;li&gt;dominat 7th chords = similar to major, with the addition of a flattened seventh above the root note of the chord (C Dominant 7th = C,E,G,bB); within a diatonic context, itâ€™s based from the 5th/dominant of any major key, so in the key of C major, the dominant 7th chord starts on G (GBDF)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Inversion: which note of the chord is placed at the bottom (root, 1st inversion, 2nd inversion)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;key-signatures&#34;&gt;Key Signatures&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Relative minor &amp;amp; major keys: the sixth degree of the major key is the relative minor key, both share exactly the same key signature (C major -&amp;gt; A minor); similarly, to find the relative major of a minor key = either count down 6 notes, or count up to the 3rd note.&lt;/li&gt;
&lt;li&gt;To add a flat sign, count up four notes from root; to add a sharp, count up five notes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;chord-progressions&#34;&gt;Chord Progressions&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Labelling:

&lt;ul&gt;
&lt;li&gt;C Major Root Position: C = I&lt;/li&gt;
&lt;li&gt;C Major 1st Inversion: C/E = Ib = I6 = I6/3&lt;/li&gt;
&lt;li&gt;C Major 2nd Inversion: C/G = Ic = I6/4&lt;/li&gt;
&lt;li&gt;G7 1st Inversion: G7/B = Vb = V6/5&lt;/li&gt;
&lt;li&gt;G7 2nd Inversion: G7/D = Vc = V4/3&lt;/li&gt;
&lt;li&gt;G7 3rd Inversion: G7/F = Vd = V4/2&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;meter&#34;&gt;Meter&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Simple meters: the top number in the time signature is a 3 or divisible by 2 (e.g., &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;, &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;, &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;, &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;, &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;, &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;).

&lt;ul&gt;
&lt;li&gt;&lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;: simple quadruple, can be represented by a â€˜Câ€™, known as common time;&lt;/li&gt;
&lt;li&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; can be represented by â€˜Câ€™ with a vertical line through it, known as cut common time&lt;/li&gt;
&lt;li&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;: simple duple meter. A bar consists of 2 crotchet beats&lt;/li&gt;
&lt;li&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;: simple triple meter. A bar consists of 3 crotchet beats&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Compound meters: top number in the time signature is divisible by 3, by a number greater than 1 (e.g., &lt;sup&gt;6&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;, &lt;sup&gt;9&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;, &lt;sup&gt;12&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;). The rhythmic value that is defined in the bottom number of the time signature, is grouped in 3s, which gives them distinct feel&lt;/li&gt;
&lt;li&gt;Beams: lines that connect shorter note values and help to clearly display the main beats of a meter.&lt;/li&gt;
&lt;li&gt;Grouping: as a general rule, the main beats of the meter must be clearly visible within the displayed rhythm at all times&lt;/li&gt;
&lt;/ul&gt;</description>
      
            <category>music</category>
      
            <category>nyu</category>
      
      
            <category>music</category>
      
            <category>notes</category>
      
    </item>
    
    <item>
      <title>A simple archive for music generation</title>
      <link>http://nianze.tk/2018/05/music-generation-archive/</link>
      <pubDate>Thu, 31 May 2018 16:07:41 -0400</pubDate>
      
      <guid>http://nianze.tk/2018/05/music-generation-archive/</guid>
      <description>&lt;p&gt;A random collection of recent works on music generation.

&lt;!-- toc --&gt;&lt;/p&gt;

&lt;h1 id=&#34;papers&#34;&gt;Papers&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Melody Generation for Pop Music via Word Representation of Musical Properties&lt;/strong&gt; (2017.10) [&lt;a href=&#34;https://arxiv.org/abs/1710.11549&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mil-tokyo/NeuralMelody&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating Nontrivial Melodies for Music as a Service&lt;/strong&gt; (2017.10) [&lt;a href=&#34;https://arxiv.org/abs/1710.02280&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://composing.ai&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MuseGAN: Symbolic-domain Music Generation and Accompaniment with Multi-track Sequential Generative Adversarial Networks&lt;/strong&gt; (2017.9) [&lt;a href=&#34;https://arxiv.org/abs/1709.06298&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://salu133445.github.io/musegan/&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity Embedding Network for Unsupervised Sequential Pattern Learning by Playing Music Puzzle Games&lt;/strong&gt; ï¼ˆ2017.9ï¼‰[&lt;a href=&#34;https://arxiv.org/abs/1709.04384&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://remyhuang.github.io/DJnet&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A Tutorial on Deep Learning for Music Information Retrieval&lt;/strong&gt; (2017.9) [&lt;a href=&#34;https://arxiv.org/abs/1709.04396&#34;&gt;arXiv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Learning Techniques for Music Generation - A Survey&lt;/strong&gt; (2017.9) &lt;a href=&#34;è®ºæ–‡ç»¼è¿°&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.01620&#34;&gt;arXiv&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural Translation of Musical Style&lt;/strong&gt; (2017.8) [&lt;a href=&#34;https://arxiv.org/abs/1708.03535&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;http://imanmalik.com/cs/2017/06/05/neural-style.html&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures&lt;/strong&gt; (2017.7) [&lt;a href=&#34;https://arxiv.org/abs/1707.04588&#34;&gt;arXiv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning and Evaluating Musical Features with Deep Autoencoders&lt;/strong&gt; (2017.6) [&lt;a href=&#34;https://arxiv.org/abs/1706.04486&#34;&gt;arXiv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models&lt;/strong&gt; (2017.5) [&lt;a href=&#34;https://arxiv.org/abs/1705.10843&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gablg1/ORGAN&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions&lt;/strong&gt; - &lt;strong&gt;ISMIR 2017&lt;/strong&gt; (2017.3) [&lt;a href=&#34;https://arxiv.org/abs/1703.10847&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automatic Conversion of Pop Music into Chiptunes for 8-bit Pixel Art&lt;/strong&gt; - &lt;strong&gt;ICASSP 2017&lt;/strong&gt; (2017.2) [&lt;a href=&#34;http://mac.citi.sinica.edu.tw/~yang/pub/su17icassp_8bit.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LemonATsu/pop-to-8bit&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://lemonatsu.github.io&#34;&gt;Page&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepBach: a Steerable Model for Bach Chorales Generation&lt;/strong&gt; (2016.12) [&lt;a href=&#34;https://arxiv.org/abs/1612.01010&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Ghadjeres/DeepBach&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;C-RNN-GAN: Continuous Recurrent Neural Networks with Adversarial Training&lt;/strong&gt; (2016.11) [&lt;a href=&#34;https://arxiv.org/abs/1611.09904&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://github.com/olofmogren/c-rnn-gan&#34;&gt;Code&lt;/a&gt;] ðŸŒŸ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tuning Recurrent Neural Networks with Reinforcement Learning - ICLR 2017&lt;/strong&gt; (2016.11) [&lt;a href=&#34;https://arxiv.org/abs/1611.02796&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner&#34;&gt;Code&lt;/a&gt;] ðŸŒŸ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient&lt;/strong&gt; - &lt;strong&gt;AAAI 2017&lt;/strong&gt; (2016.9) [&lt;a href=&#34;http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LantaoYu/SeqGAN&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Song From PI: A Musically Plausible Network for Pop Music Generation&lt;/strong&gt; - &lt;strong&gt;ICLR 2017&lt;/strong&gt; [&lt;a href=&#34;https://arxiv.org/abs/1611.03477&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;http://www.theregister.co.uk/2016/11/11/ai_pop_music_maker/&#34;&gt;Reports&lt;/a&gt;]ðŸŒŸ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Text-based LSTM networks for Automatic Music Composition&lt;/strong&gt; (2016.4) [&lt;a href=&#34;https://arxiv.org/abs/1604.05358#&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://keunwoochoi.wordpress.com/2016/02/23/lstmetallica/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/keunwoochoi/LSTMetallica&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Music Transcription Modelling and Composition Using Deep Learning&lt;/strong&gt; (2016.4) [&lt;a href=&#34;https://arxiv.org/abs/1604.08723&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IraKorshunova/folk-rnn&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composing A Melody with Long-short Term Memory (LSTM) Recurrent Neural Networks&lt;/strong&gt; (2016.2) [&lt;a href=&#34;http://konstilackner.github.io/LSTM-RNN-Melody-Composer-Website/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/konstilackner/LSTM-RNN-Melody-Composer&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;http://konstilackner.github.io/LSTM-RNN-Melody-Composer-Website/Thesis_final01.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural Adaptive Sequential Monte Carlo - NIPS 2015&lt;/strong&gt; (2015) [&lt;a href=&#34;http://papers.nips.cc/paper/5961-neural-adaptive-sequential-monte-carlo.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A Recurrent Latent Variable Model for Sequential Data - NIPS 2015&lt;/strong&gt; (2015) [&lt;a href=&#34;http://papers.nips.cc/paper/5653-a-recurrent-latent-variable-model-for-sequential-data.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jych/nips2015_vrnn&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Methods in Algorithmic Composition: A Comprehensive Survey&lt;/strong&gt; (2013) [&lt;a href=&#34;http://www.jair.org/media/3908/live-3908-7454-jair.pdf&#34;&gt;Paper&lt;/a&gt;] ðŸŒŸ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modeling Temporal Dependencies in High-dimensional Sequences: Application to Polyphonic Music Generation and Transcription&lt;/strong&gt; (2012) [&lt;a href=&#34;https://arxiv.org/abs/1206.6392&#34;&gt;arXiv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Towards Adaptive Music Generation By Reinforcement Learning of Musical Tension&lt;/strong&gt; (2010) [&lt;a href=&#34;https://ccrma.stanford.edu/~slegroux/affect/pubs/SMC2010.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A First Look at Music Composition using LSTM Recurrent Neural Networks&lt;/strong&gt; (2002) [&lt;a href=&#34;http://www.iro.umontreal.ca/~eckdoug/blues/index.html&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;http://www.iro.umontreal.ca/~eckdoug/blues/IDSIA-07-02.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;projects&#34;&gt;Projects&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Google Magenta&lt;/strong&gt; [&lt;a href=&#34;https://magenta.tensorflow.org/welcome-to-magenta&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tensorflow/magenta&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Jazz&lt;/strong&gt;  [&lt;a href=&#34;https://deepjazz.io/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://deepjazz.io/&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BachBot&lt;/strong&gt; [&lt;a href=&#34;http://bachbot.com/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/feynmanliang/bachbot/&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet&lt;/strong&gt; [&lt;a href=&#34;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&#34;&gt;Web&lt;/a&gt;]&lt;a href=&#34;not fully&#34;&gt;&lt;a href=&#34;https://github.com/ibab/tensorflow-wavenet&#34;&gt;Code&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRUV&lt;/strong&gt; [&lt;a href=&#34;https://github.com/MattVitelli/GRUV&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kulitta&lt;/strong&gt; [&lt;a href=&#34;https://github.com/donya/Kulitta&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;applications&#34;&gt;Applications&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AIVA&lt;/strong&gt;[&lt;a href=&#34;http://aiva.ai&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google A.I. Duet&lt;/strong&gt; [&lt;a href=&#34;https://aiexperiments.withgoogle.com/ai-duet&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Infinite Drum Machine&lt;/strong&gt; [&lt;a href=&#34;https://aiexperiments.withgoogle.com/drum-machine&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Amper Music&lt;/strong&gt; [&lt;a href=&#34;https://www.ampermusic.com/app#/&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intelligent Music System&lt;/strong&gt; [&lt;a href=&#34;http://120.52.72.53/www.intelligentmusicsystems.com/c3pr90ntc0td/vid/tempo_shifting.mp4&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unwind&lt;/strong&gt; [&lt;a href=&#34;http://unwind.ai&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tidalcycles&lt;/strong&gt; [&lt;a href=&#34;https://tidalcycles.org&#34;&gt;Link&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=xoa3OT8ncX0&#34;&gt;Video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jukedeck&lt;/strong&gt; [&lt;a href=&#34;https://www.jukedeck.com/&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conferences-workshops&#34;&gt;Conferences&amp;amp;Workshops&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ACM MM&lt;/strong&gt; - ACM MultiMedia [&lt;a href=&#34;http://www.acmmm.org/2017&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ISMIR&lt;/strong&gt; - The International Society of Music Information Retrieval [&lt;a href=&#34;http://www.ismir.net/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICASSP&lt;/strong&gt; - Conference on Acoustics, Speech and Signal Processing [&lt;a href=&#34;http://www.ieee-icassp2017.org/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DLM&lt;/strong&gt; - Deep Learning for Music Workshop [&lt;a href=&#34;http://dorienherremans.com/dlm2017/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CSMC&lt;/strong&gt; - Conference on Computer Simulation of Musical  Creativity [&lt;a href=&#34;https://csmc2016.wordpress.com/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CCRMA&lt;/strong&gt; - Center for Computer Research in Music and Acoustics (Stanford University) [&lt;a href=&#34;https://ccrma.stanford.edu/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ICMC&lt;/strong&gt; - Internatonal Computer Music Conference [&lt;a href=&#34;http://www.icmc2017.com/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;http://www.icmc2017.com/cn/page1.html&#34;&gt;Lists&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;blogs&#34;&gt;Blogs&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural Nets for Generating Music&lt;/strong&gt; [&lt;a href=&#34;https://medium.com/@kcimc/neural-nets-for-generating-music-f46dffac21c0&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative Music with JavaScript &amp;amp; Web Audio&lt;/strong&gt; [&lt;a href=&#34;https://teropa.info/generative-music-slides/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Current State Of AI: Artificial Intelligence In Music, Movies &amp;amp; More&lt;/strong&gt; (2017.7) [&lt;a href=&#34;http://www.hypebot.com/hypebot/2017/07/ai-today-the-current-state-of-artificial-intelligence.html&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composing Music With Recurrent Neural Networks&lt;/strong&gt; (2015.8) [&lt;a href=&#34;http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/&#34;&gt;Web&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hexahedria/biaxial-rnn-music-composition&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analyzing deep learning tools for music generation&lt;/strong&gt; [&lt;a href=&#34;http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;COORD&lt;/strong&gt; [&lt;a href=&#34;http://www.coord.fm/home/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How evolved LSTMS improvise on a melogy you specify&lt;/strong&gt;[&lt;a href=&#34;https://www.sentient.ai/sentient-labs/ea/lstm-music/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI makes pop music in the style of any composer&lt;/strong&gt;[&lt;a href=&#34;http://www.flow-machines.com/ai-makes-pop-music/&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Richard Yang&amp;rsquo;s Blog&lt;/strong&gt;[&lt;a href=&#34;https://richardyang40148.github.io/TheBlog/index.html&#34;&gt;Blog&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thousands of bird sounds visualized using machine learning&lt;/strong&gt;[&lt;a href=&#34;https://experiments.withgoogle.com/bird-sounds&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Music AI: Loop-in-the-Human&lt;/strong&gt;[&lt;a href=&#34;https://medium.com/@jayhardesty/music-ai-loop-in-the-human-1a15681e573e&#34;&gt;Web&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</description>
      
            <category>music</category>
      
            <category>recourse</category>
      
      
            <category>music</category>
      
            <category>notes</category>
      
    </item>
    
  </channel>
</rss>